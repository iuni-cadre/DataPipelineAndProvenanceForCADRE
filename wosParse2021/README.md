To run the spark scala scripts for converting WoS XML gz files to parquet you
first need to set up your envirnment properly.

1. The graphframes package is not part of the normal MVN repo, so you need
to get it from the Spark MVN repo:

   mvn org.apache.maven.plugins:maven-dependency-plugin:2.1:get
   -Dartifact=graphframes:graphframes:0.7.0-spark2.4-s_2.11
   -DrepoUrl=https://repos.spark-packages.org


You will also need to set the following environment variables:
```
export HADOOP_USER_NAME=hdfs
export PATH=~/.local/bin:$PATH
export JAVA_HOME=/usr/java/jdk1.8.0_181-cloudera/
export HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop
export HADOOP_CONF_DIR=/opt/cloudera/parcels/CDH-6.3.1-1.cdh6.3.1.p0.1470567/lib/spark/conf/yarn-conf
export SPARK_HOME=/opt/cloudera/parcels/CDH-6.3.1-1.cdh6.3.1.p0.1470567/lib/spark
```
Environmental Varaibles for PySpark Console
```
export PYTHONPATH=/N/soft/rhel7/python/3.6.8/bin/python
export PYSPARK_PYTHON=$PYTHONPATH
export PYSPARK_DRIVER_PYTHON=$PYTHONPATH
```
To execute a scala script:
```
spark-shell \\
  --master yarn \\
  --packages \\
    com.databricks:spark-xml_2.11:0.12.0,graphframes:graphframes:0.7.0-spark2.4-s_2.11 \\
  --driver-memory 8G \\
  --num-executors 21 \\
  --executor-memory 28G \\
  --executor-cores 4 \\
  --conf spark.yarn.executor.memoryOverheadFactor=0.1 \\
  --conf spark.driver.maxResultSize=8g \\
  -i <scala_script>
```
To execute a python script:
```
spark-submit \
  --master yarn \
  --driver-memory 8G \
  --num-executors 21 \
  --executor-cores 4 \
  --executor-memory 28G \
  --conf spark.yarn.executor.memoryOverheadFactor=0.2 \
  --conf spark.driver.maxResultSize=8g \
  WoSXMLParseFromParquet.py
```
Adjust memory limits, number of threads, and number of executors as needed.


The ETL pipeline for converting the source XML WoS data for ingestion by
JanusGraph is executed in several steps.

1. The source XML files containing the WoS data must first be copied to the
   Hadoop distributed file system (HDFS).  Use the 'hdfs dfs -copyFromLocal'
   command to copy the gzip files to HDFS.  The HDFS destination of the
   gzip files is customarily of the form '/WoSraw_<year>'.  The parquet files
   are customiarly stored at the HDFS location '/WoSraw_<year>/parquet'

2. The compressed XML files should be converted to parquet files for more
   efficient access during each stage of the ETL pipeline.  Alter the
   hardcoded paths in WoScompactTable.scala to specify the HDFS locations
   of the gzip files and the HDFS destinations of the parquet files.  The
   conversion to parquet format is performed by executing the shell script
   'launch_WoscompactTable.bash'.  Make sure the environment variables set
   by the script are updated for the current Hadoop and Spark installation.

3. Once the parquet format files are available the JanusGraph paper node
   data is generated using the WoSXMLParseFromParquet.py script.  Update the
   path the parquet files and the nodes files.  The node data will be in TSV
   format and split across 100 TSV files.  To perform the node generation,
   execute the shell script 'launch_WoSXMLParseFromParquet.bash'. 

4. To generate the edges files, first update the hardcoded paths in the
   'WoSCreateCitationEdges.scala' file.  To perform the edge generation,
   execute the shell script 'launch_WoSCreateCitationEdges.scala'.

5. Copy the resulting node and edge partition files from HDFS to either a
   local drive or a network file system such as geode or slate.
   1. >hdfs dfs -copyToLocal /WoSraw_2021/nodes/*.csv <destination/nodes>
   2. >hdfs dfs -copyToLocal /WoSraw_2021/edges/*.csv <destination/edges>

6. Aggregate the node data into a single TSV file and perform final formatting
   changes to the TSV.

   1. Use the 'cat' utility to combine the node partition files generated by
      Spark into a single TSV file without headers or quote marks.  Call this
      file 'nodes_without_header.tsv'

      > cat path/to/nodes/part-*-*.csv | grep -v wosID -h | sed -e 's/"//g' > nodes_without_header.tsv

   2. Extract the header without quotes from one of the node partition files.
      > head -n 1 path/to/nodes/part-00000-*.csv | sed -e 's/"//g' > node_header.tsv

   3. Concatenate the header and the nodes.
      > cat node_header.tsv nodes_without_header.tsv > nodes.tsv

7. Aggregate the edge data into a single TSV file and perform final formatting
   changes to the TSV.

   1. Use 'cat' utility to combine the edge partition files generated by Spark
      into a single TSV file without headers.  Call this file 'edges_with_headers'.
      
      > cat path/to/edges/part-*-*.csv | grep -v citing -h > edges_without_header.tsv

   2. Extract the header from one of the edge partition files.
      > head -n 1 path/to/edges/part-00000-*.csv > edge_header.tsv

   3. Concatenate the edge header and the edges.
      > cat edge_header.tsv edges_without_header.tsv > refEdges.tsv

8. Copy the node.tsv and refEdges.tsv to the /data/wos/wos_2021 directory
   on the Janus WoS ingester server (janus-wos-server-1) in AWS development subnet.      

9. Check and make sure the server properties in ~/ingester/resource/ingest-wos-cql-es-server.properties
   are pointing to the desired JanusGraph cluster.

10. Log in to a Cassandra server and check the status of the Cassandra cluster:
   > nodetool status

   Note: UN means Up and Normal.  All of the Cassandra nodes in the cluster
   should appear in the status list.

11. Start CQL shell on Cassandra node
    eg:
      > cqlsh 10.0.1.195

12. Drop all WoS data from Cassandra cluster:
   1. List current keyspaces:
      > SELECT * FROM system_schema.keyspaces;
   2. DROP KEYSPACE wosgraph;
   3. DROP KEYSPACE configurationmanagementgraph;
      - This is needed to reset connection configuration and gremlin/janusgraph connections

13. Log into the ElasticSearch server and view the current indices.
    eg:
    > curl '10.0.1.29:9200/_cat/indices?v'

14. Drop the WoS-related indices:
    >curl -X DELETE 'http://10.0.1.29:9200/janusgraph_woskeywordplusmixed'
    >curl -X DELETE 'http://10.0.1.29:9200/janusgraph_woslcstandardnamesmixed'
    >curl -X DELETE 'http://10.0.1.29:9200/janusgraph_wospubyearmixed'
    >curl -X DELETE 'http://10.0.1.29:9200/janusgraph_wosabstractmixed'
    >curl -X DELETE 'http://10.0.1.29:9200/janusgraph_wospapertitlemixed'
    >curl -X DELETE 'http://10.0.1.29:9200/janusgraph_wosjournaltitlemixed'

    Note: The file delete_wos_indices.bash in the home directory performs this.
          Make sure the list of indices in step 14 matches the list of WoS
          indices to be deleted.

15. Log into the ingester VM (usually named janus-wos-server-1 or simiarly)

16. Set up the environment to run the ingester:
    > export JANUSGRAPH_HOME=~/janusgraph

17. The ingestion process is configured using a few different files.
    The file to configure the number of workers and batch sizes for data to
    be ingested are in /home/ubuntu/ingester/janusgraph-utils/conf.
    The schema and mappings from TSV files to JanusGraph properties are
    in ~/ingester/resource/wos_maps.

    Modify batch import at ~/home/ubuntu/ingester/janusgraph-utils/conf/batch_import.properties
    a. workers - number of workers for reading data and sending to cluster nodes
    b. workers.target_record_count - Number of records to be processed by a worker at a time
    c. vertex.record_commit_count - Number of vertex records to be committed to the database at a time
    d. edge.record_commit_count - Number of edge records to be committed to the database at at time

    Current JanusGraph cluster setup works best with two workers.  More than two workers causes
    slowdowns and lost connections and dropped transactions in the backend systems.

18. Modify the mappings and schema properties, if needed. The files
    are located in ~/ingester/resource/wos_maps.  The subdirectories are
    for edge-only, vertex-only, and vertex-and-edge ingestions:
    1. edges
    2. vertices
    3. vertices_edges

    The file WOS_schema.json lists the column properties for tables and the
    indices used by ElasticSearch for fast, indexed access.  The file
    WOS_datamapper defines the mappings between TSV headers and the
    property names; this allows different header names to be in the TSV
    file and mapped to the correct property names.  Make sure these
    mappings are correct.

19. The script to start the ingestion is at: ~/ingester/janusgraph-utils/run.sh.
    In this step we show how to create the schema and ingest both vertex and
    edge files.

    1. Create the schema.
       >./run.sh loadsch ~/ingester/resource/ingest-wos-cql-es-server.properties ~/ingester/resource/wos_maps/vertices_edges/WOS_schema.json ~/ingester/resource/wos_maps/vertices_edges/WOS_datamapper.json
    2. Log into the ElasticSearch server and modify the index configuration for lc_standard_names.
       This field can sometimes be longer than the indexer can process and needs to be truncated for
       a relative handful of paper vertices.  The following command limits the number of bytes
       processed by the indexer.  Without this restriction, the ingester will throw several exceptions.

       > curl -H 'Content-Type: application/json' -X POST '10.0.1.29:9200/janusgraph_woslcstandardnamesmixed/_mapping/wosLcStandardNamesMixed' -d '
         {
            "properties": {
            "lc_standard_names__STRING": {
            "type": "keyword",
            "ignore_above": 32766
         }
       }
     }' 

    3. Check to make sure the new configuration for the wosLcStandardNamesMixed
       is in effect:
       > curl -X GET '10.0.1.29:9200/janusgraph_woslcstandardnamesmixed/_mapping'

20. Start the ingester to ingest both vertex and edge data.  The node data
    is expected to be in the nodes.tsv file and the edge data is expected
    to be in the refEdges.tsv file.  The file names are specified in the
    WOS_datamapper.json file.

    > ./run.sh import ~/ingester/resource/ingest-wos-cql-es-server.properties /data/wos/wos_2021 ~/ingester/resource/wos_maps/vertices_edges/WOS_schema.json ~/ingester/resource/wos_maps/vertices_edges/WOS_datamapper.json skipSchema &> ~/log/wos/import_mmddyyyy.log &
