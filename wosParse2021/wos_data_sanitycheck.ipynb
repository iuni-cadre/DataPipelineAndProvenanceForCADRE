{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from subprocess import Popen, PIPE\n",
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runShellCmdBlocking(cmd):\n",
    "\n",
    "    process = Popen(cmd, shell = True, stdout = PIPE, stderr = PIPE)\n",
    "    \n",
    "    (output, err) = process.communicate()\n",
    "    \n",
    "    exitCode = process.wait()\n",
    "     \n",
    "    # in python3, output type is bytes, in python2, it is string\n",
    "    if output is not None: output = output.decode(\"utf-8\")\n",
    "    if err is not None: err = err.decode(\"utf-8\")\n",
    "\n",
    "    return (exitCode, output, err)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### command example ###\n",
    "# time ./run.sh import ~/ingester/resource/ingest-pto-cql-es-server.properties \n",
    "# /data/uspto/cleaned/nodes ~/ingester/resource/uspto_schema.json \n",
    "# ~/ingester/resource/uspto_datamapper.json 2>&1 | tee ~/log/uspto/ingest.log\n",
    "\n",
    "def generateTemplateAndSubmit(lookupTable, elementKey, dataMapper, templateFilepath, \n",
    "                            ingesterHome, janusgraphHome, graphConfig, schema, logDir):\n",
    "    \n",
    "    print('Processing {}'.format(elementKey))\n",
    "    \n",
    "    elementMap = dataMapper[elementKey]\n",
    "    \n",
    "    # handling nodes/edges:\n",
    "    for element in lookupTable:\n",
    "        print('Processing {}'.format(element))\n",
    "\n",
    "        folder = lookupTable[element]\n",
    "        print('Search folder {}'.format(folder))\n",
    "\n",
    "        files = glob(os.path.join(folder, '*.csv'))\n",
    "        print('Found {} files'.format(len(files)))\n",
    "\n",
    "        key = \"<\" + element.upper() + \"_FILE_PLACEHOLDER>\"\n",
    "        print('Using key {}'.format(key))\n",
    "\n",
    "        fieldMapping = elementMap[key]\n",
    "\n",
    "        for file in files:\n",
    "            dataMapDict = {}\n",
    "            dataMapDict[elementKey] = {}\n",
    "            \n",
    "            basename = os.path.basename(file)\n",
    "\n",
    "            dataMapDict[elementKey][basename] = fieldMapping\n",
    "         \n",
    "            print('Writing to {}'.format(templateFilepath))\n",
    "            \n",
    "            with open(templateFilepath, 'w') as f:\n",
    "                json.dump(dataMapDict, f)\n",
    "\n",
    "            print('Generated template at {}'.format(templateFilepath))\n",
    "            \n",
    "            \n",
    "            logFile = element + '_' + os.path.splitext(basename)[0] + '.log'\n",
    "            \n",
    "            logFilepath = os.path.join(logDir, logFile)\n",
    "            #submit job\n",
    "            cmdList = [\n",
    "                'cd ' + ingesterHome,\n",
    "                'export JANUSGRAPH_HOME=' + janusgraphHome,\n",
    "                'time ./run.sh import ' + graphConfig + ' ' + \\\n",
    "                    folder + ' ' + schema + ' ' + \\\n",
    "                    templateFilepath + ' 2>&1 | tee ' + logFilepath]\n",
    "      \n",
    "            commands = ';'.join(cmdList)\n",
    "            \n",
    "            print(commands)\n",
    "            \n",
    "\n",
    "def mergeData(inputdir, outputdir):\n",
    "    \n",
    "    globPattern = inputdir + os.sep + \"*\" + os.sep\n",
    "    \n",
    "    folders = [x for x in os.listdir(inputdir) if os.path.isdir(os.path.join(inputdir, x))]\n",
    "    \n",
    "    for folder in folders:\n",
    "            \n",
    "        print('Processing folder {}'.format(folder))\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        outfileName = folder + '.csv'\n",
    "        \n",
    "        files = glob(os.path.join(inputdir, folder, \"*.csv\"))\n",
    "        print('Found {} files'.format(len(files)))\n",
    "        \n",
    "        files.sort()\n",
    "        \n",
    "        outfilepath = os.path.join(outputdir, outfileName)\n",
    "        with open(outfilepath, 'w') as outfile:\n",
    "            \n",
    "            totalNumRows = 0\n",
    "            \n",
    "            for i in range(len(files)):\n",
    "                \n",
    "#                 print('Processing file {}'.format(files[i]))\n",
    "\n",
    "                lineOffset = 0\n",
    "                \n",
    "                with open(files[i], 'r') as infile:\n",
    "                    \n",
    "                    for line in infile:\n",
    "                        line = line.strip()\n",
    "                        line = '\\t'.join(line.split(','))\n",
    "                        \n",
    "                        if lineOffset == 0:\n",
    "                            if i == 0:\n",
    "                                \n",
    "                                outfile.writelines(line + '\\n')   \n",
    "                        else:\n",
    "                            outfile.writelines(line + '\\n')\n",
    "                            \n",
    "                        lineOffset += 1\n",
    "                        \n",
    "                totalNumRows += lineOffset - 1\n",
    "                \n",
    "            print('{} rows in outfile {}'.format(totalNumRows, outfilepath))\n",
    "                        \n",
    "        \n",
    "        print(\" ------- {} seconds --------\".format(time.time() - start))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder refEdges\n",
      "Found 100 files\n",
      "1169338266 rows in outfile /N/project/rds/CADRE/dataset/wos/edges/refEdges.csv\n",
      " ------- 8657.893625736237 seconds --------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# inputdir = '/N/project/mag/mag_jg_2021_update/nodes'\n",
    "# outputdir = '/N/project/rds/CADRE/dataset/mag/2021/combined/nodes'\n",
    "inputdir = '/N/project/mag/wos_csv/edges'\n",
    "outputdir = '/N/project/rds/CADRE/dataset/wos/edges'\n",
    "mergeData(inputdir, outputdir)\n",
    "\n",
    "# inputdir = '/N/project/mag/mag_jg_2021_update/edges'\n",
    "# outputdir = '/N/project/rds/CADRE/dataset/mag/2021/combined/edges'\n",
    "# mergeData(inputdir, outputdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sanityCheckNumCols(folder, delimiter, logDir):\n",
    "\n",
    "    files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "    \n",
    "    print('Processing folder {}'.format(folder))\n",
    "    \n",
    "    for filename in files:\n",
    "          \n",
    "        logFile = os.path.join(logDir, 'sc-' + os.path.basename(filename) + '.log')\n",
    "        with open(logFile, 'w') as logfp:\n",
    "            \n",
    "            with open(os.path.join(folder, filename), 'r') as fp:\n",
    "                logfp.write('Processing file {}\\n'.format(filename))\n",
    "                print('Processing file {}'.format(filename))\n",
    "                \n",
    "                start = time.time()\n",
    "\n",
    "                recordCnt = 0\n",
    "                numInvalidRecords = 0\n",
    "                headerSet = False\n",
    "\n",
    "                for line in fp:\n",
    "\n",
    "                    ## Note the strip() will remove all trailing whitespace characters like\n",
    "                    ## \\t\\t\\t\\n if parameter not given, so we need to specify explicitly\n",
    "                    ## that we only want to remove new line character\n",
    "                    line = line.strip('\\n')\n",
    "\n",
    "                    if not headerSet:\n",
    "                        logfp.write('Header:\\n{}\\n'.format(line))\n",
    "                        print('Header:\\n{}'.format(line))\n",
    "\n",
    "                        numCols = len(line.split(delimiter))\n",
    "                        logfp.write('# of columns: {}\\n'.format(numCols))\n",
    "                        print('# of columns: {}'.format(numCols))\n",
    "\n",
    "                        headerSet = True\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        recordCnt += 1\n",
    "\n",
    "                        fields = line.split(delimiter)\n",
    "                        numFields = len(fields)\n",
    "                        if numFields != numCols:\n",
    "                            numInvalidRecords += 1 \n",
    "                            \n",
    "#                             i = 0\n",
    "#                             for field in fields:\n",
    "#                                 i = i + 1\n",
    "#                                 print('Field {}:{}'.format(i, field))\n",
    "\n",
    "                            logfp.write('line # {} below, expected # cols {}, actual got {}\\n{}\\n'.\\\n",
    "                                  format(recordCnt + 1, numCols, numFields, line))\n",
    "                            print('line # {} below, expected # cols {}, actual got {}\\n{}'.\\\n",
    "                                  format(recordCnt + 1, numCols, numFields, line))\n",
    "                            \n",
    "#                     if recordCnt >= 1:\n",
    "#                         break\n",
    "\n",
    "                logfp.write('Finished processing file {}\\n'.format(filename))\n",
    "                print('Finished processing file {}, '.format(filename))\n",
    "                logfp.write('# of invalid records: {}/{}\\n'.format(numInvalidRecords, recordCnt))\n",
    "                print('# of invalid records: {}/{}'.format(numInvalidRecords, recordCnt))\n",
    "                print(\" ------- {} seconds --------\".format(time.time() - start))\n",
    "                    \n",
    "                 \n",
    "def sanityCheckColLength(folder, delimiter, colIdx, sizeThreshold, logDir):\n",
    "\n",
    "    files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "    \n",
    "    print('Processing folder {}'.format(folder))\n",
    "    \n",
    "    for filename in files:\n",
    "          \n",
    "        logFile = os.path.join(logDir, 'sc-' + os.path.basename(filename) + '.log')\n",
    "        with open(logFile, 'w') as logfp:\n",
    "            \n",
    "            with open(os.path.join(folder, filename), 'r') as fp:\n",
    "                logfp.write('Processing file {}\\n'.format(filename))\n",
    "                print('Processing file {}'.format(filename))\n",
    "                \n",
    "                start = time.time()\n",
    "\n",
    "                recordCnt = 0\n",
    "                numInvalidRecords = 0\n",
    "                headerSet = False\n",
    "\n",
    "                for line in fp:\n",
    "\n",
    "                    ## Note the strip() will remove all trailing whitespace characters like\n",
    "                    ## \\t\\t\\t\\n if parameter not given, so we need to specify explicitly\n",
    "                    ## that we only want to remove new line character\n",
    "                    line = line.strip('\\n')\n",
    "\n",
    "                    if not headerSet:\n",
    "                        logfp.write('Header:\\n{}\\n'.format(line))\n",
    "                        print('Header:\\n{}'.format(line))\n",
    "\n",
    "                        numCols = len(line.split(delimiter))\n",
    "                        logfp.write('# of columns: {}\\n'.format(numCols))\n",
    "                        print('# of columns: {}'.format(numCols))\n",
    "\n",
    "                        headerSet = True\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        recordCnt += 1\n",
    "\n",
    "                        fields = line.split(delimiter)\n",
    "                        \n",
    "                        targetField = fields[colIdx]\n",
    "                        \n",
    "                        size = len(targetField.encode('utf-8'))\n",
    "                        if size >= sizeThreshold:\n",
    "                            numInvalidRecords += 1                           \n",
    "\n",
    "                            logfp.write('line # {}, lc_standard_names {}, size {}\\n'.\\\n",
    "                                  format(recordCnt + 1, targetField, size))\n",
    "#                             print('line # {}, wosid {}, standardnames {}, lc_standard_names {}, size {}\\n'.\\\n",
    "#                                   format(recordCnt + 1, fields[0], fields[-15],targetField, size))\n",
    "#                             print('line # {}, wosid {}, standardnames {}, size {}\\n'.\\\n",
    "#                                   format(recordCnt + 1, fields[0], fields[-15], size))\n",
    "                            \n",
    "#                             break\n",
    "#                     if recordCnt >= 1:\n",
    "#                         break\n",
    "\n",
    "                logfp.write('Finished processing file {}\\n'.format(filename))\n",
    "                print('Finished processing file {}, '.format(filename))\n",
    "                logfp.write('# of invalid records: {}/{}\\n'.format(numInvalidRecords, recordCnt))\n",
    "                print('# of invalid records: {}/{}'.format(numInvalidRecords, recordCnt))\n",
    "                print(\" ------- {} seconds --------\".format(time.time() - start))\n",
    "                \n",
    "                \n",
    "def filterByLength(folder, delimiter, colIdx, sizeThreshold, outDir):\n",
    "\n",
    "    files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "    \n",
    "    print('Processing folder {}'.format(folder))\n",
    "    \n",
    "    for filename in files:\n",
    "        \n",
    "        filenameWithoutExt = os.path.splitext(filename)[0]\n",
    "        remainFile = os.path.join(outDir, filenameWithoutExt + '_filtered.tsv')\n",
    "        exludeFile = os.path.join(outDir, filenameWithoutExt + '_excluded.tsv')\n",
    "        \n",
    "        with open(remainFile, 'w') as fpRemain:\n",
    "            with open(exludeFile, 'w') as fpExclude:\n",
    "                with open(os.path.join(folder, filename), 'r') as fp:\n",
    "                  \n",
    "                    print('Processing file {}'.format(filename))\n",
    "\n",
    "                    start = time.time()\n",
    "\n",
    "                    recordCnt = 0\n",
    "                    numInvalidRecords = 0\n",
    "                    headerSet = False\n",
    "\n",
    "                    for line in fp:\n",
    "\n",
    "                        ## Note the strip() will remove all trailing whitespace characters like\n",
    "                        ## \\t\\t\\t\\n if parameter not given, so we need to specify explicitly\n",
    "                        ## that we only want to remove new line character\n",
    "                        line = line.strip('\\n')\n",
    "\n",
    "                        if not headerSet:\n",
    "                            print('Header:\\n{}'.format(line))\n",
    "\n",
    "                            numCols = len(line.split(delimiter))\n",
    "                            print('# of columns: {}'.format(numCols))\n",
    "\n",
    "                            headerSet = True\n",
    "\n",
    "                            fpRemain.write(line + '\\n')\n",
    "                            fpExclude.write(line + '\\n')\n",
    "                        else:\n",
    "\n",
    "                            recordCnt += 1\n",
    "\n",
    "                            fields = line.split(delimiter)\n",
    "\n",
    "                            targetField = fields[colIdx]\n",
    "\n",
    "                            size = len(targetField.encode('utf-8'))\n",
    "                            if size >= sizeThreshold:\n",
    "                                numInvalidRecords += 1                           \n",
    "\n",
    "                                fpExclude.write(line + '\\n')\n",
    "                            else:\n",
    "                                fpRemain.write(line + '\\n')\n",
    "\n",
    "\n",
    "                print('Finished processing file {}, '.format(filename))\n",
    "\n",
    "                print('# of invalid records: {}/{}'.format(numInvalidRecords, recordCnt))\n",
    "                print(\" ------- {} seconds --------\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.0\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder /N/project/mag/wos_csv/fromPostgres\n",
      "Processing file sixth_export.tsv\n",
      "Header:\n",
      "wosid\tisopenaccess\topenaccesstype\tabstract\tfundingtext\tcitedreferencecount\tfull_address\treprintaddress\tarticlenumber\tpublicationyear\tpublicationdate\tvolume\tissue\tpartnumber\tsupplement\tspecialissue\tearlyaccessdate\tstartpage\tendpage\tnumberofpages\tpublishercity\tpublisheraddress\tpublisher\tkeywordplus\tconferencedate\tconferencesponsor\tconferencehost\tconferencetitle\tdocumenttype\trids\torcid\tstandardnames\tauthors\temailaddress\tpapertitle\tjournaltitle\tjournalabbrev\tjournaliso\tissn\tdoi\teissn\tisbn\tpmid\tconferencelocation\tfundingorgs\tlc_standard_names\n",
      "# of columns: 46\n",
      "Finished processing file sixth_export.tsv, \n",
      "# of invalid records: 0/78395307\n",
      " ------- 477.3129072189331 seconds --------\n"
     ]
    }
   ],
   "source": [
    "folderList = [\n",
    "#     '/N/project/rds/CADRE/dataset/mag/2021/combined/nodes',\n",
    "#     '/N/project/rds/CADRE/dataset/mag/2021/combined/edges'\n",
    "#     '/N/project/mag/wos_csv'\n",
    "#    '/N/project/mag/wos_csv/tmp/'\n",
    "    '/N/project/mag/wos_csv/fromPostgres'\n",
    "]\n",
    "\n",
    "delimiter = '\\t'\n",
    "logDir = 'output'\n",
    "\n",
    "for folder in folderList:\n",
    "    sanityCheckNumCols(folder, delimiter, logDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder /N/project/mag/wos_csv/fromPostgres\n",
      "Processing file sixth_export.tsv\n",
      "Header:\n",
      "wosid\tisopenaccess\topenaccesstype\tabstract\tfundingtext\tcitedreferencecount\tfull_address\treprintaddress\tarticlenumber\tpublicationyear\tpublicationdate\tvolume\tissue\tpartnumber\tsupplement\tspecialissue\tearlyaccessdate\tstartpage\tendpage\tnumberofpages\tpublishercity\tpublisheraddress\tpublisher\tkeywordplus\tconferencedate\tconferencesponsor\tconferencehost\tconferencetitle\tdocumenttype\trids\torcid\tstandardnames\tauthors\temailaddress\tpapertitle\tjournaltitle\tjournalabbrev\tjournaliso\tissn\tdoi\teissn\tisbn\tpmid\tconferencelocation\tfundingorgs\tlc_standard_names\n",
      "# of columns: 46\n",
      "Finished processing file sixth_export.tsv, \n",
      "# of invalid records: 2040/78395307\n",
      " ------- 604.5593001842499 seconds --------\n"
     ]
    }
   ],
   "source": [
    "folderList = [\n",
    "#     '/N/project/rds/CADRE/dataset/mag/2021/combined/nodes',\n",
    "#     '/N/project/rds/CADRE/dataset/mag/2021/combined/edges'\n",
    "#     '/N/project/mag/wos_csv'\n",
    "#    '/N/project/mag/wos_csv/tmp/'\n",
    "    '/N/project/mag/wos_csv/fromPostgres'\n",
    "]\n",
    "\n",
    "delimiter = '\\t'\n",
    "logDir = 'output'\n",
    "\n",
    "colIdx = -1\n",
    "sizeThreshold = 32766\n",
    "\n",
    "for folder in folderList:\n",
    "    sanityCheckColLength(folder, delimiter, colIdx, sizeThreshold, logDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder /N/project/mag/wos_csv/fromPostgres\n",
      "Processing file sixth_export.tsv\n",
      "Header:\n",
      "wosid\tisopenaccess\topenaccesstype\tabstract\tfundingtext\tcitedreferencecount\tfull_address\treprintaddress\tarticlenumber\tpublicationyear\tpublicationdate\tvolume\tissue\tpartnumber\tsupplement\tspecialissue\tearlyaccessdate\tstartpage\tendpage\tnumberofpages\tpublishercity\tpublisheraddress\tpublisher\tkeywordplus\tconferencedate\tconferencesponsor\tconferencehost\tconferencetitle\tdocumenttype\trids\torcid\tstandardnames\tauthors\temailaddress\tpapertitle\tjournaltitle\tjournalabbrev\tjournaliso\tissn\tdoi\teissn\tisbn\tpmid\tconferencelocation\tfundingorgs\tlc_standard_names\n",
      "# of columns: 46\n",
      "Finished processing file sixth_export.tsv, \n",
      "# of invalid records: 2040/78395307\n",
      " ------- 729.5439267158508 seconds --------\n",
      "CPU times: user 8min 42s, sys: 3min 27s, total: 12min 10s\n",
      "Wall time: 12min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "folderList = [\n",
    "#     '/N/project/rds/CADRE/dataset/mag/2021/combined/nodes',\n",
    "#     '/N/project/rds/CADRE/dataset/mag/2021/combined/edges'\n",
    "#     '/N/project/mag/wos_csv'\n",
    "#    '/N/project/mag/wos_csv/tmp/'\n",
    "    '/N/project/mag/wos_csv/fromPostgres'\n",
    "]\n",
    "\n",
    "delimiter = '\\t'\n",
    "outDir = '/N/project/mag/wos_csv/fromPostgres'\n",
    "\n",
    "colIdx = -1\n",
    "sizeThreshold = 32766\n",
    "\n",
    "for folder in folderList:\n",
    "    filterByLength(folder, delimiter, colIdx, sizeThreshold, outDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sanityCheckQuotationMarks(folder, delimiter, logDir):\n",
    "\n",
    "    files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "    \n",
    "    print('Processing folder {}'.format(folder))\n",
    "    \n",
    "    for filename in files:\n",
    "        \n",
    "        if filename not in ['sixth_export.tsv']:\n",
    "            continue\n",
    "            \n",
    "        logFile = os.path.join(logDir, 'sc-' + os.path.basename(filename) + '.log')\n",
    "        with open(logFile, 'w') as logfp:\n",
    "            \n",
    "            with open(os.path.join(folder, filename), 'r') as fp:\n",
    "\n",
    "                logfp.write('Processing file {}\\n'.format(filename))\n",
    "                print('Processing file {}'.format(filename))\n",
    "\n",
    "                start = time.time()\n",
    "\n",
    "                recordCnt = 0\n",
    "                numInvalidRecords = 0\n",
    "                headerSet = False\n",
    "\n",
    "                for line in fp:\n",
    "\n",
    "                    line = line.strip('\\n')\n",
    "\n",
    "                    if not headerSet:\n",
    "                        logfp.write('Header:\\n{}\\n'.format(line))\n",
    "#                         print('Header:\\n{}'.format(line))\n",
    "\n",
    "                        numCols = len(line.split(delimiter))\n",
    "                        logfp.write('# of columns: {}\\n'.format(numCols))\n",
    "#                         print('# of columns: {}'.format(numCols))\n",
    "\n",
    "                        headerSet = True\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        recordCnt += 1\n",
    "\n",
    "                        fields = line.split(delimiter)\n",
    "                        for field in fields:\n",
    "\n",
    "                            if field.startswith('\"') or field.endswith('\"'):\n",
    "                                logfp.write('line # {} below, field(s) with unremoved quotation mark\\n{}\\n'.\\\n",
    "                                  format(recordCnt + 1, line))\n",
    "                                print('line # {} below, field(s) with unremoved quotation mark\\n{}'.\\\n",
    "                                  format(recordCnt + 1, line))\n",
    "                                numInvalidRecords += 1\n",
    "                                break\n",
    "\n",
    "                    if numInvalidRecords >= 5:\n",
    "                        break\n",
    "\n",
    "            print('Finished processing file {}'.format(filename))\n",
    "            print('# of invalid records: {}/{}'.format(numInvalidRecords, recordCnt))\n",
    "            print(\" ------- {} seconds --------\".format(time.time() - start))\n",
    "            \n",
    "            logfp.write('Finished processing file {}\\n'.format(filename))\n",
    "            logfp.write('# of invalid records: {}/{}\\n'.format(numInvalidRecords, recordCnt))\n",
    "            logfp.write(\" ------- {} seconds --------\\n\".format(time.time() - start))\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder /N/project/mag/wos_csv/fromPostgres\n",
      "Processing file sixth_export.tsv\n",
      "Finished processing file sixth_export.tsv\n",
      "# of invalid records: 0/78395307\n",
      " ------- 1833.9017338752747 seconds --------\n"
     ]
    }
   ],
   "source": [
    "folderList = [\n",
    "#     '/N/project/rds/CADRE/dataset/mag/2021/combined/nodes',\n",
    "#     '/N/project/rds/CADRE/dataset/mag/2021/combined/edges'\n",
    "#     '/N/project/mag/wos_csv'\n",
    "#     '/N/project/mag/wos_csv/tmp/'\n",
    "    '/N/project/mag/wos_csv/fromPostgres'\n",
    "]\n",
    "\n",
    "delimiter = '\\t'\n",
    "\n",
    "logDir = 'output'\n",
    "\n",
    "for folder in folderList:\n",
    "    sanityCheckQuotationMarks(folder, delimiter, logDir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
